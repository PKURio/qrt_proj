{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e17551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "import time\n",
    "from dataset import NormalDataset\n",
    "from model import SelfAttnModel,GinAttnModel,EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86934d7",
   "metadata": {},
   "source": [
    "## get necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e759922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df_stage1=pd.read_csv(\"./qube/data/ashares_daily_stage1.csv\")\n",
    "    df_stage1_lite=df_stage1[[\"norm_wma_open\",\"norm_wma_close\",\"norm_wma_high\",\"norm_wma_low\",\"diff_log_vol\",\n",
    "                             \"f1\",\"f2\",\"f3\",\"trade_date\",\"return\",\"stock_id\"]]\n",
    "    np_stage1_lite=np.array(df_stage1_lite)\n",
    "    dict_colname_lite={}\n",
    "    for i,colname in enumerate(df_stage1_lite.columns.values.tolist()):\n",
    "        dict_colname_lite[colname]=i\n",
    "    return df_stage1_lite,dict_colname_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4643ee7",
   "metadata": {},
   "source": [
    "## loss & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5712562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcc_loss(x,y):\n",
    "    x=x.squeeze()\n",
    "    y=y.squeeze()\n",
    "    x_hat=th.mean(x)\n",
    "    y_hat=th.mean(y)\n",
    "    return 1-th.mean((x-x_hat)*(y-y_hat))/(th.std(x)*th.std(y))\n",
    "\n",
    "def ccc_loss(x,y):\n",
    "    x=x.squeeze()\n",
    "    y=y.squeeze()\n",
    "    x_hat=th.mean(x)\n",
    "    y_hat=th.mean(y)\n",
    "    return 1-2*th.mean((x-x_hat)*(y-y_hat))/(th.std(x)**2+th.std(y)**2+(x_hat-y_hat)**2)\n",
    "\n",
    "def extra_ic(x,y):\n",
    "    ic=[format(1-pcc_loss(x[:,i],y).item(),\".4f\") for i in range(x.shape[-1])]\n",
    "    print(f\"[TEST] extra ic {ic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a8f82",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8226bb7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttnModel(\n",
      "  (emb_id): Embedding(1605, 3)\n",
      "  (mlp1): Linear(in_features=8, out_features=32, bias=False)\n",
      "  (mlp2): Linear(in_features=192, out_features=32, bias=False)\n",
      "  (mlp3): Linear(in_features=32, out_features=10, bias=False)\n",
      "  (fc): Linear(in_features=10, out_features=1, bias=False)\n",
      "  (act1): Sequential(\n",
      "    (0): PReLU(num_parameters=1)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (act2): Sequential(\n",
      "    (0): PReLU(num_parameters=1)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (act3): Sequential(\n",
      "    (0): PReLU(num_parameters=1)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(32, 32, batch_first=True, dropout=0.3)\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "[TRAIN] epoch 1 total loss 34.79574555158615 elapsed time 35.51377558708191s\n",
      "[TEST] extra ic ['0.0580', '-0.0175', '0.0575', '0.0461', '0.0314', '-0.0272', '0.0408', '0.0583', '0.0667', '-0.0712']\n",
      "[TEST] epoch 1 ic 0.051871299743652344 elapsed time 4.415899753570557s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train(np_stage1_lite,dict_colname_lite):\n",
    "    df_stage1_lite,dict_colname_lite=get_data()\n",
    "    \n",
    "    GPU_VIS=0\n",
    "    th.manual_seed(0)\n",
    "    th.cuda.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    th.cuda.set_device(GPU_VIS)\n",
    "\n",
    "    seq_len=5\n",
    "    train_ranges=(20100000,20200000)\n",
    "    test_ranges=(20200000,20210000)\n",
    "    train_bs,test_bs=int(1e5),int(1e5)\n",
    "\n",
    "    train_set=NormalDataset(seq_len,np_stage1_lite,train_ranges,dict_colname_lite)\n",
    "    test_set=NormalDataset(seq_len,np_stage1_lite,test_ranges,dict_colname_lite)\n",
    "    train_dataset=DataLoader(train_set,batch_size=train_bs,num_workers=4,pin_memory=True,shuffle=True)\n",
    "    test_dataset=DataLoader(test_set,batch_size=test_bs,num_workers=4,pin_memory=True,shuffle=False)\n",
    "\n",
    "    EPOCH=1\n",
    "    LR=1e-3\n",
    "    DECAY=0.999\n",
    "\n",
    "    args={\n",
    "        \"dropout\":[0.1,0.3,0.3],\n",
    "        \"in_dim\":8,\n",
    "        \"dim_emb_id\":3,\n",
    "        \"dim_lstm_hid\":32,\n",
    "    }\n",
    "\n",
    "    model=SelfAttnModel(**args).to(device=\"cuda\")\n",
    "    print(model)\n",
    "\n",
    "    # ema=EMA(model,DECAY)\n",
    "    # ema.register()\n",
    "\n",
    "    optimizer=th.optim.AdamW(model.parameters(),lr=LR,weight_decay=1e-3)\n",
    "    def loss_func(x,y):\n",
    "        return 0*nn.MSELoss()(x,y)+ccc_loss(x,y)\n",
    "\n",
    "    scaler=GradScaler()\n",
    "    for epoch in range(EPOCH):\n",
    "        t1=time.time()\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for idx,(src,stock_id,label) in enumerate(train_dataset):\n",
    "            src=src.cuda(non_blocking=True)\n",
    "            stock_id=stock_id.cuda(non_blocking=True)\n",
    "            label=label.cuda(non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output=model(src,stock_id).flatten()\n",
    "                loss=loss_func(output,label)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    #         ema.update() \n",
    "            total_loss+=loss.item()\n",
    "\n",
    "        t2=time.time()\n",
    "        print(f\"[TRAIN] epoch {epoch+1} total loss {total_loss} elapsed time {t2-t1}s\")\n",
    "    #     ema.apply_shadow()\n",
    "        model.eval()\n",
    "        all_pred=th.zeros(int(1e6)).cuda()\n",
    "        all_label=th.zeros(int(1e6)).cuda()\n",
    "        all_extra=th.zeros(int(1e6),10).cuda()\n",
    "        with th.no_grad():\n",
    "            cnt=0\n",
    "            for idx,(src,stock_id,label) in enumerate(test_dataset):\n",
    "                src=src.cuda(non_blocking=True)\n",
    "                stock_id=stock_id.cuda(non_blocking=True)\n",
    "                label=label.cuda(non_blocking=True)\n",
    "\n",
    "    #             pred=model(src,stock_id).flatten()\n",
    "                pred,extra=model(src,stock_id,True)\n",
    "                pred=pred.flatten()\n",
    "                pred_len=pred.shape[0]\n",
    "                all_pred[cnt:cnt+pred_len]=pred\n",
    "                all_label[cnt:cnt+pred_len]=label\n",
    "                all_extra[cnt:cnt+pred_len]=extra\n",
    "                cnt+=pred_len\n",
    "\n",
    "            all_pred=all_pred[:cnt]    \n",
    "            all_label=all_label[:cnt]\n",
    "            all_extra=all_extra[:cnt]\n",
    "            ic_test=1-pcc_loss(all_pred,all_label).item()\n",
    "            extra_ic(all_extra,all_label)\n",
    "\n",
    "        t3=time.time()\n",
    "        print(f\"[TEST] epoch {epoch+1} ic {ic_test} elapsed time {t3-t2}s\")\n",
    "        print(f\"{'-'*100}\")\n",
    "    #     ema.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
